0-
Questo progetto ha l'obiettivo di testare le capacità agentiche delle inteligenze artificiali attualmente in commercio. Il concetto di AI agentiche cioè sistemi autonomi in grado di eseguire tasks senza l'intervento umano, ha sdoganato l'utilizzo dei Large Language Models per attività per cui non sono stati progettati. Infatti, in quanto pappagalli stocastici, estraggono sulla base di pattern riconosciuti autonomamente alcune parole dal proprio dataset di riferimento senza comprenderne il significato.
Per controllarne la robustezza in questi nuovi compiti sono stati introdotti dei benchmark agentici che indagano la capacità di risolvere problemi afferenti al mondo reale.
Sulla stessa scia questa tesi si propone di testare la robustezza di alcuni LLMs, con una disivione categorica chiara per aiutarne l'utilizzo consapevole.

1-
Nello specifico è stato possibile testare 3 prominenti LLMs: Gemini, Gemma e Llama. Con l'obiettivo di creare un benchmark per tre specifiche categorie di problemi: Reasoning, Factuality e Sequential Problem Solving.

2-
Gemini è un modello proprietario di Google, le versioni più recenti sono della famiglia 2.5. L'utilizzo è stato posibile tramite le API fornite, la versione più recente per cui è stato possibile testare l'intero benchmark è la 2.0 flash. Gemini possiede una context window di circa 1 milione di tokens che nel concreto significa poter considerare un testo della lunghezza di Moby Dick per fornire una risposta e utilizza un modello sparse Mixture of Expert.
Sparse MoE consiste nell'attivare per ogni token un subset dei parametri del modello specializzati nel contesto del token. Per semplificare è simile a creare diversi modelli ognuno specializzato in ambiti differenti e attivare solo quelli coerenti per ogni token.
Gemma è una famiglia di modelli open-source realizzati da Google. I parametri del modello sono quantizzati a 1, 4, 12 e 27 Bilion parameters (abbreviato in B). Maggiore è il numero di parametri e maggiori sono le risorse hardware richieste per eseguirlo. In questo benchmark il modello migliore utilizzato è il 4B.
Llama, invece, è un modello open-source di Meta. Anche questo modello è parametrizzato ed è stato possibile testare quelli quantizzati a 1 e 3 B mentre non è stato possibile eseguire quelli di 11 e 90 B.

3-
Passiamo ora allo specifico dell'esperimento eseguito:
L'accessi alle API Gemini è stato effettuato tramite 13 account free tier in parallelo per oltrepassare il rate-limit imposto.
Dei tre LLMs presentati sono state testate 10 diverse versioni.
Il benchmark è composto da 22 domande, che sono state sottoposte molteplici volte utilizzando diverse metodologie di prompting. Queste 22 domande ricadono in 9 tipologie di problemi che a loro volta rientrano in 3 macro-categorie.
Le risposte totali ottenute sono poco più 273 mila.
)
4-
Qui mostriamo alcuni esempi di domande appartenenti a diverse categorie: 
della categoria Reasoning e tipologia Mathematical Reasoning: calcolare il punto di massimo della funzione y=x*sin(x) nell'intervallo tra 0 e pigreco.
in Factuality con tipologia Factual pifalls: quanti sono i commi dell'articolo 140 della costituzione italiana. (gli articoli della costituzione sono 139)

5-
Le 3 categorie presentate: Reasoning, Factuality e Sequential Problem Solving raggruppano diverse tipologie di problemi. I nomi delle tipologie di problemi sono autoesplicativi. 
Bisogna però fare una distinzione, in Reasoning, tra Mathematical Reasoning che comprende problemi particolarmente complessi di matematica o afferenti al modo reale e Common Math Problems che riguarda problemi dalla difficoltà calmierata.
Invece Russel's Theory of Description nella categoria Factuality è una teoria filosofica per cui se una domanda è posta con un assunzione fallace allora in alcuni casi non possa considerarsi nè corretta nè errata, un esempio: "l'attuale re di Francia è morto" è un affermazione che prevede di descrive una persona non esistente in quanto la Francia è una repubblica e quindi non è considerabile nè vera nè falsa. Le domande in questa categoria studiano la capacità logica di eseguire questa distinzione.

6-
L'ultima categoria: Sequential Problem Solving prende ispirazione dall'anomalia di Sussman che studia il problema della gestione di piani sovrapposti nei problemi di planning risolti con AI.
L'errore ricorrente avviene quando l'AI dividendo il problema in sotto-problemi propone delle sotto-soluzioni che si annullano a vicenda.
Come si può anche vedere nelle risposte grezze dello studio, spesso la soluzione proposta è un ciclo infinito di due operazioni opposte che impediscono il progredire della risoluzione.
Questa categoria ottiene dei risultati pessimi nel benchmark, suggerendo una difficoltà nel risolvere problemi di planning complesso da parte degli LLMs.

7- L'interrogazione degli LLMs è avvenuta con metodologie differenti:
Gemini è stato acceduto tramite le API esposte da Google, utilizzando 13 account 'free-tier' in parallelo per superare il rate-limit imposto, mentre Gemma e Llama sono stati eseguiti localmente utilizzando Ollama: un software che ne semplifica il deployment in differenti sistemi operativi. 
I modelli hostati localmente hanno un basso numero di parametri a causa dell'hardware utilizzato. Per effettuare i test sui modelli di dimensioni maggiori in tempi adeguati, servirebbe una GPU con prestazioni migliori e una VRAM più capiente data l'alta richiesta di risorse.

8- Le domande sono state poste utilizzando differenti strategie di prompting.
Con la metodologia One Shot è stato richiesto di fornire unicamente la risposta al problema.
Utilizzando zero-shot Chain of Thought tramite una chat si è prima chiesto di ragionare sul problema e risolverlo step-by-step, in secondo luogo la richiesta è stata di raffinare l'output fornendo solo la risposta al problema.
Con Programming of thought si è chiesto all'agente di scrivere un codice python che risolvesse il problema. Questo codice è stato poi eseguito esternamente tramite un exec con un timeout adeguato, così da catturare eventuali errori.

9- L'intero processo di interrogazione agli agenti è stato automatizzato tramite script python che indirizzava le richieste alle API google o a Ollama e salvava le risposte in un file di testo.
Da questo file le risposte sono state confrontate con le soluzioni, tendo conto di un margine di errore prestabilito e poi inserite in un file CSV.
Tramite alcune macro in Basic i risultati sono stati aggregati e trasformati nei grafici che visualizzeremo in seguito.

10- I dati di questi grafici sono stati elaborati in 3 passaggi:
per iniziare, per ogni domanda posta è stata calcolata la correttezza sulle diversi ripetizioni eseguite dallo stesso account.
In secondo luogo, la mediana dell'accuratezza è stata calcolata tra i diversi account che in Gemini hanno posto la stessa domanda, quindi dalla singola domanda l'accuratezza passa a livello degli account.
In conclusione la media di queste mediane è stata calcolata tra le domande di ogni categoria di problemi. Ottenendo in questo modo un'aggregazione a livello della tipologia di problema.

[Aggiugi qualcosa nelle prossime slide, aggiungi una slide con la storicità di Gemini?]

11- Per semplicità, in seguito sono riportati i risultati del benchmark unicamente per i modelli più recenti tra quelli testati e con un maggiore numero di parametri.

Nella versione più recente, tra quelle testate, di Gemini possiamo notare:
- Il miglioramento dei risultato con l'applicazione della metodologia PoT per i problemi matematici più semplici mentre un peggioramento su quelli maggiormente complessi.
  In questa presentazione non sono riportati i modelli precedenti di Gemini altrimenti sarebe possibile notare un progressivo miglioramento in queste prime due categorie.
- I problemi del Sudoku, invece, sono risultati completamente errati in tutti i modelli, questo compreso. I risultati ottenuti possono essere un esempio di allucinazione perchè tutti i risultati forniti avevano una certa coerenza con le regole del sudoku ma erano completamente inventati.
- I trabocchetti su domande di fattualità, a loro volta ottengono un accuratezza dello zero percento. Questo risultato mostra la difficoltà dell'agente nel contraddire le affermazione dell'interlocutore.
- Nella teoria delle descrizioni di Russel i risultati sono stati discreti unicamente nell'ultima versione ma le motivazioni fornite nelle risposte mostrano la macananza del concetto filosofici.
- Le 4 restanti categorie di problemi sono inquadrate in Sequential Problem Solving ed ottengono in tutte le versioni dei risultati con accuratezza molto bassa mostrando l'estrema difficoltà a gestire problemi di planning con piani sovrapposti, nonostante nel progredire delle versioni di Gemini i risultati siano andati leggermente migliorando.

12- I modelli presentati a segure: Llama e Gemma, hanno risultati decisamente inferiori anche perchè scontano lo scarso numero di parametri che è stato possibile utilizzare a causa delle limitazioni hardware.

Anche in questo caso nei problemi matematici più semplici l'utilizzo di PoT ha permesso di ottenere degli ottimi risultati se confrontati con quelli delle altre metodologie.
Inoltre nella teoria delle descrizioni di Russel il miglioramento dei risultati è discordante tra versioni, numero di parametri e domande lasciando credere che i buoni risultati siano randomici.
Per i problemi maggiormente complessi invece, si possono vedere risultati molto prossimi allo 0% di accuratezza.

13- Anche per gemma i commenti che si possono fare sul benchmark risultante sono molto simili a quelli di Llama. In questo modello appare più evidente la difficolta nella teoria delle descrizioni ed è facile da riconoscere il miglioramento della risoluzione dei problemi matematici più semplici tramite l'utilizzo di PoT.
Per i problemi più complessi i risultati sono sempre prossimi allo zero ma si può intravedere la tendenza nell'ottenere risultati migliori con la metodologia CoT rispetto a PoT con problemi matematici più complessi.
